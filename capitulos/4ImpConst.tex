\chapter{Implementação}\label{cap:implementacao}

Neste capítulo são apresentados os detalhes da implementação das funcionalidades propostas neste trabalho, incluindo a contrução da camada de interoperabilidade, a implementação da biblioteca padrão em RL e as adaptações realizadas no \textit{frontend} do compilador Robcmp.

O código-fonte completo pode ser encontrado no repositório online \url{https://github.com/LuizEduardoRezende/robcmp/tree/tflm-front-end}. O referido repositório é um \textit{fork} do projeto Robcmp original, e todo o desenvolvimento pode ser encontrado na \textit{branch} \texttt{tflm-front-end}. Futuramente, será submetido um \textit{pull request} para integrar estas contribuições à \textit{branch} principal do projeto Robcmp, a fim de que as novas funcionalidades fiquem disponíveis para a comunidade.

\section{Arquitetura da Camada de Interoperabilidade}\label{sec:arq-camada-compatibilidade}
O processo de compilação e linkedição do Robcmp pode ser explicado da seguinte forma: programas escritos com a extensão \texttt{.rob} são compilados pelo Robcmp e geram arquivos objeto (\texttt{.o}). Já as bibliotecas externas, como o TFLM ou outras, são compiladas separadamente, resultando em arquivos de biblioteca estática (\texttt{.a}). Após a compilação de todos os programas e bibliotecas necessários, o \textit{linker} entra em ação com o objetivo de resolver todas as referências entre os componentes e criar um executável único, que pode ser chamado de \textit{firmware}.

É válido ressaltar que a biblioteca estática do TFLM pode ser facilmente compilada para diferentes arquiteturas, utilizando o sistema de \texttt{Makefile} do proprio \textit{framework}. Diferente do arquivo objeto do programa em RL, que é gerado pelo próprio compilador Robcmp. Por fim, a ponte entre a RL e o TFLM é feita pelo \textit{wrapper} C, que também é compilado por um sistema de Maekfile dentro do diretório \textsl{test/tflm-tests}, gerando sua própria biblioteca estática.

A \hyperref[fig:linker]{Figura~\ref*{fig:linker}} ilustra a arquitetura de compilação e interoperabilidade entre os componentes, as caixas azuis representam os códgios fontes dos diferentes componentes, enquanto que as caixas verdes representam os artefatos gerados após a compilação de cada componente. Após serem compilados, o linker unifica todos os artefatos em um único executável final.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{Imagens/linker.png}
\caption{Ilustração da arquitetura de compilação e interoperabilidade. As bibliotecas estáticas (\texttt{.a}) do TFLM e do \textit{wrapper} C, assim como o arquivo objeto (\texttt{.o}) do programa em RL, são processados de forma independente e, na etapa final, unificados pelo \textit{linker}. O \textit{linker} resolve as referências entre os componentes para criar o executável único.}
\label{fig:linker}
\end{figure}

\section{Implementação do \textit{wrapper} C}
O principal objetivo do \textit{wrapper} C é fornecer uma interface simples e direta para que o código gerado pela RL possa interagir com a API do TFLM. Embora o arquivo-fonte tenha a extensão \texttt{.cpp} para permitir a chamada direta à API C++ do TFLM, todas as funções de interface foram declaradas com \texttt{extern “C”} para garantir uma ligação C (\textit{C linkage}) pura. Essa abordagem permite que o código compilado da RL se conecte ao \textit{wrapper} como se ele fosse uma biblioteca C nativa, resolvendo a interoperabilidade em tempo de linkagem. O código do \textit{wrapper} pode ser encontrado no repositório do Robcmp, na pasta \texttt{/wrappers} ou no link direto \url{https://github.com/LuizEduardoRezende/robcmp/blob/tflm-front-end/wrappers/tflm/tflm_wrapper.cpp}.

\subsection{Estrutura \texttt{TFLM\_Instance}}
A estrutura \texttt{TFLM\_Instance} encapsula todos os objetos principais do TFLM, o interpretador, o alocador de memória e o resolvedor de operações. Isso mantém o estado de cada modelo carregado de forma organizada.

\lstdefinestyle{cpp}{
    language=C++,
    backgroundcolor=\color{listbggray},
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{red},
    numbers=left,
    stepnumber=1,
    tabsize=4,
    showstringspaces=false,
    breaklines=true,
    captionpos=t,
    morekeywords={uintptr_t, int8_t, uint8_t, size_t, int32_t, round, roundf}
}

\begin{lstlisting}[style=cpp, caption={Struct \texttt{TFLM\_Instance} encontrada no começo do \textit{wrapper}.}, label={lst:tflm-instance}]
struct TFLM_Instance {
    tflite::MicroInterpreter* interpreter;
    tflite::MicroAllocator* allocator;       
    MutableResolver* resolver;
};
\end{lstlisting}

\subsection{Tipo enumerado \texttt{KernelType} e função \texttt{RegisterOp}}
A enumeração \texttt{KernelType}, inspirada na enumeração \texttt{BuiltinOperator} do próprio TFLM, foi criada com o intuito de possibilitar o registro dinâmico de operadores (\textit{kernels}) necessários para a execução de um modelo. Tanto a enumeração quanto a função \texttt{RegisterOp} atuam em conjunto para garantir que apenas os \textit{kernels} essenciais, como \texttt{CONV\_2D} ou \texttt{FULLY\_CONNECTED}, sejam carregados dinamicamente conforme a necessidade especificada pelo desenvolvedor. 

A função \texttt{RegisterOp} recebe como parâmetros o resolvedor de operações do TFLM e o tipo do \textit{kernel} a ser registrado. Utilizando uma estrutura de controle \texttt{switch-case}, a função verifica o tipo do \textit{kernel} e chama o método apropriado do resolvedor para registrá-lo. Além disso, ela contém diretivas de compilação condicional para incluir ou excluir o registro de determinados \textit{kernels} com base em flags de compilação, como \texttt{ENABLE\_ADD} ou \texttt{ENABLE\_AVERAGE\_POOL\_2D}. Caso o \textit{kernel} solicitado não esteja habilitado ou não seja suportado, a função imprime uma mensagem de aviso.

\begin{lstlisting}[style=cpp, caption={Tipo enumerado \texttt{KernelType} e função \texttt{RegisterOp}.}, label={lst:kernel-type}]
typedef enum {
  ADD = 0,
  AVERAGE_POOL_2D = 1,
  CONCATENATION = 2,
  CONV_2D = 3,
  DEPTHWISE_CONV_2D = 4,
  /* ... demais operadores ... */
} KernelType;

void RegisterOp(tflite::MicroMutableOpResolver<TFLM_MAX_OPS>* resolver, KernelType kernel_type) {
    switch (kernel_type) {
        
#if ENABLE_ADD
        case ADD:
            resolver->AddAdd();
            MicroPrintf("ADD registrado");
            break;
#endif

#if ENABLE_AVERAGE_POOL_2D
        case AVERAGE_POOL_2D:
            resolver->AddAveragePool2D();
            MicroPrintf("AVERAGE_POOL_2D registrado");
            break;
#endif
    /* ... demais casos ... */
        default:
            MicroPrintf("!!! Kernel %d nao suportado ou nao habilitado na compilacao", kernel_type);
            break;
    }
}
\end{lstlisting}

\subsection{Configuração de Kernels: \texttt{kernel\_config.h}}
Para facilitar a configuração dos \textit{kernels} disponíveis no \textit{wrapper}, foi criado o arquivo de cabeçalho \texttt{kernel\_config.h}. Nesse arquvio é possível habilitar ou desabilitar o suporte a cada \textit{kernel} individualmente, através de diretivas de pré-processamento. Cada \textit{kernel} possui uma diretiva \texttt{\#define ENABLE\_<NOME\_DO\_KERNEL>} conforme mostra o \hyperref[lst:kernel-config]{Código~\ref*{lst:kernel-config}}. Além disso, é possível habilitar ou desabilitar os logs de debug do TFLM, que são úteis para diagnóstico e depuração durante o desenvolvimento.

A diretiva \texttt{\#pragma once} na primeira linha garante que o arquivo seja processado apenas uma vez pelo compilador evitando erros de duplicidade de definições.

Esse tipo de otimização é crucial em sistemas embarcados, pois permite reduzir o tamanho final do binário (\textit{footprint}) ao excluir o código de operadores que não serão utilizados pelo modelo ou desabilitar os logs de debug. Isso garante que apenas os recursos estritamente necessários sejam linkados ao \textit{firmware} final.

\begin{lstlisting}[style=cpp, caption={Arquivo de configuração \texttt{kernel\_config.h}.}, label={lst:kernel-config}]
#pragma once

// Configuracao de logging
//1 = Habilitar logs de debug (MicroPrintf)
//0 = Desabilitar logs de debug
#define ENABLE_DEBUG_LOGS 0

// Configuracao de kernels
//1 = Habilitar kernel
//0 = Desabilitar kernel
#define ENABLE_ADD 0
#define ENABLE_AVERAGE_POOL_2D 1
#define ENABLE_CONCATENATION 0
#define ENABLE_CONV_2D 1
/* ... demais kernels ... */
\end{lstlisting}   

\subsection{\texttt{InitializeInterpreter}}
É a primeira função a ser chamada em um fluxo de inferência, responsável por configurar o interpretador e outras instâncias necessárias para a execução do modelo, sua assinatura é apresentada no \hyperref[lst:initialize-interpreter]{Código~\ref*{lst:initialize-interpreter}}. Recebe como parâmetros os dados do modelo em formato de array de bytes, o buffer de memória pré-alocado (tensor arena) e a lista de \textit{kernels} a serem registrados. Retorna um identificador único (\textit{handle}) para a instância do modelo, do tipo \texttt{uintptr\_t}, um inteiro sem sinal cujo tamanho varia de acordo com a arquitetura para a qual o \textit{wrapper} foi compilado. Um inteiro é retornado porque o Robcmp não possui suporte a ponteiros nativos em sua linguagem.

É importante ressaltar que a função recebe alguns parâmetros adicionais além dos já mencionados, como \texttt{tensor\_arena\_size}, \texttt{num\_kernels} e um parâmetro sem nome. Esses parâmetros podem ou não ser utilizados, mas devem constar na declaração da função por questões de compatibilidade com o \textit{backend} do LLVM. As demais funções do \textit{wrapper} também podem conter parâmetros extras pelo mesmo motivo.

A necessidade desses parâmetros surge quando o Robcmp utiliza a biblioteca padrão do diretório \texttt{lib/ai/tflm.rob}, que contém as declarações das funções do wrapper conforme mostrado na \hyperref[lst:get-tensor-value]{Seção~\ref*{sec:bib-padrao}}. Essas declarações precisam estar alinhadas com as definições reais das funções no \textit{wrapper}. No arquivo \texttt{.rob}, para cada argumento que seja um vetor, é adicionado um argumento extra do tipo inteiro que representa o tamanho desse vetor. Isso é uma característica do compilador Robcmp para tratar vetores em chamadas de função. Portanto, para manter a consistência entre declarações e definições, esses parâmetros adicionais são incluídos nas definições das funções no \textit{wrapper}.

No caso de uso do \textit{wrapper} sem a biblioteca padrão \texttt{tflm.rob}, ou seja, quando as chamadas são geradas diretamente pelo \textit{frontend} do compilador, esses parâmetros adicionais não são necessários, pois as chamadas ocorrem diretamente no código C++ ou no LLVM-IR gerado pela análise semântica do compilador.

\begin{lstlisting}[style=cpp, caption={Assinatura da função \texttt{InitializeInterpreter}.}, label={lst:initialize-interpreter}]
uintptr_t InitializeInterpreter(const uint8_t* model_data,
                               uint8_t* tensor_arena,
                               const uint8_t* required_kernels,
                               int,
                               int tensor_arena_size,
                               int8_t num_kernels);
\end{lstlisting}

\subsection{\texttt{GetInputTensor} e \texttt{GetOutputTensor}}
Estas são funções simples que retornam ponteiros para os tensores de entrada e saída do modelo, respectivamente. Esses ponteiros são utilizados para leitura e escrita dos dados posteriormente. As assinaturas das funções são mostradas no \hyperref[lst:get-tensor]{Código~\ref*{lst:get-tensor}}, elas recebem como parâmetro o \textit{handle} do modelo e também o index do tensor desejado. Modelos podem ter mais de um tensor de entrada ou saída, ou seja , um modelo pode retornar diferentes arrays de saída dependendo da sua arquitetura.

\begin{lstlisting}[style=cpp, caption={Assinatura das funções \texttt{GetInputTensor} e \texttt{GetOutputTensor}.}, label={lst:get-tensor}]
uintptr_t GetOutputTensor(uintptr_t instance_handle, size_t index, int);

uintptr_t GetInputTensor(uintptr_t instance_handle, size_t index, int);
\end{lstlisting}


\subsection{Convenção de Tipos de Dados e Quantização}
Modelos de TinyML frequentemente usam tipos de dados otimizados, como inteiros de 8 bits, para economizar memória e acelerar o processamento. O \textit{wrapper} simplifica isso para o usuário da RL, na medida em que permite que o desenvolvedor trabalhe sempre com valores de ponto flutuante (\texttt{float}), independentemente do tipo de dado real utilizado pelo modelo. Isso se mostra útil em situações onde os dados são recebidos de um sensor em ponto flutuante como temperatura, pressão ou aceleração. 

As funções \texttt{SetTensorArray} e \texttt{SetTensorValue} do \hyperref[lst:get-tensor-value]{Código~\ref*{lst:get-tensor-value}} convertem automaticamente valores de ponto flutuante para o formato quantizado exigido pelo modelo e vice-versa. O desenvolvedor em RL pode trabalhar sempre com float, e o \textit{wrapper} cuida de toda a matemática de conversão internamente. Para tensores quantizados, o \textit{wrapper} aplica o mapeamento linear baseado nos parâmetros de escala (\texttt{scale}) e ponto zero (\texttt{zero\_point}) do tensor, conforme as formulas presentes no \hyperref[lst:get-quant]{Código~\ref*{lst:get-quant}}.

Apesar dessa estrátegia de deixar o trabalho todo para o \textit{wrapper} cobrir grande partes dos casos de uso, existem situações em que o modelo espera dados já quantizados, ou seja, dados já em inteiros de 8 bits. Nesses casos, a função \texttt{SetTensorFromIntArray} permite que o desenvolvedor forneça diretamente um array de inteiros, evitando a necessidade de conversão para ponto flutuante e posterior quantização. Isso é útil quando os dados já estão no formato quantizado, como em casos onde os dados são coletados de sensores que fornecem leituras em inteiros ou quando os dados são pré-processados externamente antes de serem passados ao modelo. 

Para essas situações, a função \texttt{SetTensorFromIntArray} do \hyperref[lst:get-tensor-value]{Código~\ref*{lst:get-tensor-value}} aceita um array de inteiros de 16 bits, que são então tranformados para 8bits (apenas corta os bits excedentes) e copiados diretamente para o tensor, sem qualquer conversão adicional.

Por fim, para o resgate dos valores após a inferência, as funções \texttt{GetTensorAsFloat} e \texttt{GetTensorArray} também realizam a desquantização automática(caso seja necessário), convertendo os valores quantizados de volta para ponto flutuante usando as mesmas fórmulas de mapeamento linear.

\begin{lstlisting}[style=cpp,caption={Formúlas de quantização e desquantização.}, label={lst:get-quant}]
int32_t valor_quantizado = (int32_t)roundf((valor_float / scale) + (zero_point));

float valor_float = scale * (valor_quantizado - zero_point);
\end{lstlisting}

\begin{lstlisting}[style=cpp, caption={Assinatura das funções de resgate e passagem de valores.}, label={lst:get-tensor-value}]
void SetTensorValue(uintptr_t tensor_handle, size_t index, float value, int);

void SetTensorArray(uintptr_t tensor_handle, const float* values, size_t count, int);

void SetTensorFromIntArray(uintptr_t tensor_handle, const int16_t* values, size_t count, int);

float GetTensorAsFloat(uintptr_t tensor_handle, size_t index, int);

void GetTensorArray(uintptr_t tensor_handle, float* values, size_t max_count, int);
\end{lstlisting}

\subsection{\texttt{InvokeInterpreter}}
Essa é uma função simples responsável pela inferência do modelo. Ela aciona o TFLM para executar o modelo com os dados de entrada fornecidos. É o “cérebro” da operação, funcionando como um botão de ação. Sua assinatura está no \hyperref[lst:invoke]{Código~\ref*{lst:invoke}}, ela recebe apenas o \textit{handle} do modelo como parâmetro.

\begin{lstlisting}[style=cpp, caption={Assinatura da função \texttt{InvokeInterpreter}.}, label={lst:invoke}]
void InvokeInterpreter(uintptr_t instance_handle, int);
\end{lstlisting}

\subsection{\texttt{DestroyInterpreter}}
Essa é uma função simples e semelhante a anterior, é responsável por liberar os recursos alocados para a instância do modelo, evitando vazamentos de memória. Sua assinatura está no \hyperref[lst:destroy]{Código~\ref*{lst:destroy}}, ela recebe apenas o \textit{handle} do modelo como parâmetro.


\begin{lstlisting}[style=cpp, caption={Assinatura da função \texttt{DestroyInterpreter}.}, label={lst:destroy}]
void DestroyInterpreter(uintptr_t instance_handle, int);
\end{lstlisting}

\section{Fluxo de Trabalho de Inferência}
Para realizar uma inferência com um modelo TinyML utilizando o \textit{wrapper}, o fluxo de trabalho segue os seguintes passos principais, por meio das chamadas de função correspondentes:

\begin{enumerate}
    \item Inicialização (\texttt{InitializeInterpreter}): Esta é a etapa inicial do processo, onde são passados os dados necessários para configurar o modelo e é recebido um \textit{handle} que representa a instância do modelo carregado.
        
    \item Acesso aos tensores (\texttt{GetInputTensor} e \texttt{GetOutputTensor}): Com o \textit{handle} do modelo, é possível obter ponteiros para os tensores de entrada e saída, que são utilizados para leitura e escrita dos dados.
        
    \item Fornecimento dos dados de entrada (\texttt{SetTensorArray}) e (\texttt{SetTensorFromIntArray}): A partir dos \textit{handles} dos tensores, os dados de entrada são inicializados.
        
    \item Execução da inferência (\texttt{InvokeInterpreter}): Esta função é acionada para que os dados de entrada possam ser processados pelo modelo, gerando os resultados no tensor de saída.
        
    \item Obtenção dos resultados (\texttt{GetTensorAsFloat} e \texttt{GetTensorArray}): Após a inferência, essas funções são usadas para ler os resultados do tensor de saída. Elas retornam o dado em formato de ponto flutuante.

    \item Liberação de recursos (\texttt{DestroyInterpreter}): Essa função pode ser chamada para liberar os recursos para a instância do modelo, evitando vazamentos de memória.
\end{enumerate}

\section{Biblioteca padrão \texttt{tflm.rob}}\label{sec:bib-padrao}

A biblioteca padrão \texttt{tflm.rob} foi criada para servir como um passo intermediário entre a adaptação do \textit{frontend} e o desenvolvimento do \textit{wrapper} C. Ela contém as declarações de todas as funções do \textit{wrapper}, como mostrado no \hyperref[lst:bib-rob]{Código~\ref*{lst:bib-rob}}, permitindo que programas escritos em RL chamem essas funções diretamente, sem a necessidade de modificar o compilador.

Ao declarar as funções no arquivo \texttt{tflm.rob}, o compilador passa a reconhecer sua existência e possibilita chamadas a partir do código \texttt{.rob}. As implementações reais estão presentes na biblioteca estática do \textit{wrapper} C, utilizada em tempo de execução. Na etapa de linkedição, o \textit{linker} resolve as referências entre o código objeto do programa e a biblioteca estática do \textit{wrapper}, produzindo o executável final.

Além de facilitar a validação das funções do \textit{wrapper}, a biblioteca \texttt{tflm.rob} também funciona como uma forma de apresentar o trabalho desenvolvido até o momento, servindo como mecanismo de segurança caso a modificação do \textit{frontend} não esteja completa ou apresente problemas, antes da integração definitiva ao compilador.

Para utilizar alguma biblioteca padrão da RL, basta incluir no início do código a importação correspondente com a palavra chave \texttt{use} seguida do caminho relativo a pasta \texttt{lib}. No caso do TFLM, a linha de importação é \texttt{use "ai.tflm";}, uma vez que o arquivo se encontra na diretório \texttt{lib/ai/tflm.rob} do Robcmp.

\begin{lstlisting}[style=rob, caption={Biblioteca padrão \texttt{tflm.rob}}, label={lst:bib-rob}]
enum KERNELS{
  ADD = 0,
  AVERAGE_POOL_2D = 1,
  CONCATENATION = 2,
  CONV_2D = 3,
  // demais operadores...
}

// Inicializacao com kernels especificos
uint64 InitializeInterpreter(uint8[] model_data, uint8[] tensor_arena, uint8[] required_kernels);

// Destruicao e limpeza
void DestroyInterpreter(uint64 instance_handle);

// Acesso aos tensores
uint64 GetInputTensor(uint64 instance_handle, uint64 index);
uint64 GetOutputTensor(uint64 instance_handle, uint64 index);

// Manipulacao de dados de entrada dos tensores
void SetTensorValue(uint64 tensor_handle, uint64 index, float value);
void SetTensorArray(uint64 tensor_handle, float[] values, uint64 count);
void SetTensorFromIntArray(uint64 tensor_handle, int16[] values, uint64 count);

// Manipulacao de dados de saida dos tensores
float GetTensorAsFloat(uint64 tensor_handle, uint64 index);
uint64 GetTensorSize(uint64 tensor_handle);
void GetTensorArray(uint64 tensor_handle, float[] values, uint64 max_count);

// Informacoes sobre tensores
uint64 GetInputTensorCount(uint64 instance_handle);
uint64 GetOutputTensorCount(uint64 instance_handle);

// Execucao de inferencia
void InvokeInterpreter(uint64 instance_handle);

// Funcoes de Analise de modelo
void DiagnoseModel(uint8[] model_data);
void VerifyModelData(uint8[] model_data);

// Funcao simples para imprimir float com quebras de linha
void PrintFloat(float value);
\end{lstlisting}

\section{Ampliação da Robotics Language}

As novas palavras chaves da RL foram criadas com o intuíto de possibilitar uma experiência de desenvolvimento mais fluida, fácil e intuitiva para o usuário final. Com essas novas palavras chaves, o desenvolvedor consegue declarar um modelo, fornecer os dados de entrada, executar a inferência e obter os resultados de forma direta e simplificada.

O \hyperref[lst:nova-sintaxe]{Código~\ref*{lst:nova-sintaxe}} mostra um exemplo simples de utilização das novas palavras-chave. Esse exemplo baseia-se em um uso real de um modelo TinyML disponível no repositório do TFLM: um modelo de predição de valores de seno denominado ``hello\_world\_model''. Neste exemplo, primeiro são declaradas as constantes necessárias, como o tamanho da arena de memória e os \textit{kernels} utilizados pelo modelo. Em seguida, o modelo é carregado a partir do arquivo TFLite, e os dados de entrada são fornecidos diretamente ao modelo. Após isso, a inferência é executada com a palavra-chave \texttt{invoke} e, finalmente, os resultados são obtidos diretamente do modelo.

\begin{lstlisting}[style=rob, caption={Exemplo de código com a nova sintaxe para inferência de um modelo tflite.}, label={lst:nova-sintaxe}]
int16 main() {
	// 1. Define os dados de entrada, um buffer para a saida e o tamanho da arena.
	const arena_size = 8000;
	const kernels = {9};
	dados = {1.57};
    output = {0.0};

	// 2. Carrega o modelo
	model meu_modelo("meu_modelo.tflite", arena_size, kernels);

	// 3. Fornece a entrada para o modelo.
	meu_modelo.input[0] = dados;

	// 4. Executar a inferencia.
	invoke meu_modelo;
	
	// 5. Obtem o resultado.
	output = meu_modelo.output[0];
	//Resto do codigo
}
\end{lstlisting}

Para a implementação dessa nova sintaxe, foram necessárias modificações no \textit{frontend} do compilador Robcmp, especificamente nas fases de análise léxica, sintática e semântica. Todo código fonte do \textit{frontend} está localizado na pasta \texttt{src/}, portanto, todos arquivos mencionados nesta seção podem ser encontrados nesse diretório do repositório do Robcmp.

\lstdefinelanguage{Flex}{
  morekeywords={return},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
  alsoletter={_},
  emph={TOK_MODEL, TOK_INVOKE, TOK_MODEL_INPUT, TOK_MODEL_OUTPUT},
  emphstyle=\color{blue}\bfseries,
}

\lstdefinestyle{flex}{
  language=Flex,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{gray}\itshape,
  stringstyle=\color{red},
  numbers=left,
  stepnumber=1,
  breaklines=true,
  tabsize=2,
  showstringspaces=false,
}

\subsection{Análise léxica}
O arquivo Flex responsável pela análise léxica do Robcmp é o \texttt{Language.l}, cujo objetivo é identificar os tokens da linguagem RL. Foram adicionados quatro novos tokens para suportar as palavras-chave e as funcionalidades: \texttt{TOK\_MODEL}, \texttt{TOK\_INVOKE}, \texttt{TOK\_MODEL\_INPUT} e \texttt{TOK\_MODEL\_OUTPUT}.

Enquanto as expressões regulares para os tokens \texttt{TOK\_MODEL} e \texttt{TOK\_INVOKE} são simples, correspondendo diretamente às palavras-chave ``model'' e ``invoke'', as expressões para \texttt{TOK\_MODEL\_INPUT} e \texttt{TOK\_MODEL\_OUTPUT} são mais complexas: identificam padrões formados por um identificador seguido de ``.input'' ou ``.output'' e capturam o lexema completo para uso posterior nas análises sintática e semântica.

\begin{lstlisting}[style=Flex,caption={Novos \textit{tokens} do \textit{Scanner} Flex.},label={lst:language-l}]
"model"					{ return TOK_MODEL; }  // TFLM
"invoke"				{ return TOK_INVOKE; } // TFLM

{ID}\.input			{ yylval->ident = strndup(yytext, yyleng);
						  return TOK_MODEL_INPUT;
						}

{ID}\.output			{ yylval->ident = strndup(yytext, yyleng);
						  return TOK_MODEL_OUTPUT;
						}
\end{lstlisting}

\subsection{Análise sintática}
No Robcmp, os arquivos responsáveis pela análise sintática são \texttt{Language.y}, \texttt{LanguageHeader.y} e \texttt{LanguageUse.y}. O primeiro define a gramática principal da linguagem RL, enquanto os dois últimos tratam, respectivamente, das declarações de cabeçalho e das regras sintáticas para a inclusão da biblioteca padrão.

Este trabalho concentrou-se principalmente no arquivo \texttt{Language.y}, onde foram adicionadas novas regras sintáticas à gramática principal da linguagem, e também no arquivo \texttt{LanguageHeader.y}, para suportar os novos tokens e novos tipos.

Inicialmente, antes de criar as novas regras sintáticas, foi necessário adicionar os novos tokens ao arquivo \texttt{LanguageHeader.y}, conforme mostrado no  \hyperref[lst:language-header]{Código~\ref*{lst:language-header}}. Além disso, foi preciso definir um novo tipo de dado para representar os modelos TinyML, esse tipo recebeu o nome de \texttt{ModelNode}. Esse tipo foi adicionado ao arquivo \texttt{Header.y}, responsável por definir todos os tipos possíveis da AST de um programa, conforme mostrado no \hyperref[lst:language-types]{Código~\ref*{lst:language-types}}.

\begin{lstlisting}[style=Bison, caption={Novos tokens adicionados no \texttt{LanguageHeader.y}.},label={lst:language-header}]
%token TOK_MODEL TOK_INVOKE TOK_MODEL_INPUT TOK_MODEL_OUTPUT
\end{lstlisting}

\begin{lstlisting}[style=cpp,caption={Novo tipo de dado definido no arquivo \texttt{Header.h}.},label={lst:language-types}]
#include "ModelNode.h"
\end{lstlisting}

Após a fase de preparação, as novas regras sintáticas são definidas, juntamente com os novos tokens e seus respectivos tipos de dados no arquivo \texttt{Language.y}. Conforme mostrado no código \hyperref[lst:language-rules]{Código~\ref*{lst:language-rules}}, primeiramente os tokens são declarados e seus tipos de dado são definidos. Em seguida, as novas regras são implementadas na posição apropriada da gramática. A regra de declaração de um modelo (\texttt{model\_stmt}) é adicionada às produções \texttt{global} e \texttt{stmt}, permitindo que modelos sejam declarados tanto no escopo global quanto dentro de funções ou blocos. Por outro lado, a regra de invocação de um modelo (\texttt{invoke\_stmt}) é adicionada apenas à produção \texttt{stmt}, já que invocações de modelos não fazem sentido no escopo global. Além disso, as regras para manipulação de tensores são incorporadas nas produções \texttt{factor} e \texttt{complexvar\_set}, permitindo que tensores de entrada e saída sejam tratados como variáveis complexas e como expressões, respectivamente.

Depois que o compilador reconhece uma sequencia de tokens que se encaixa nessas novas regras sintáticas, ele cria os nós correspondentes na AST. Esses nós são instâncias da classe \texttt{ModelNode}, que encapsula todas as informações relevantes sobre o modelo TinyML, como o nome do modelo, o tipo de operação (entrada, saída ou invocação), os parâmetros associados e a localização no código-fonte para fins de depuração.

\begin{lstlisting}[style=Bison,caption={Declaração dos tokens e das regras sintáticas no \texttt{Language.y}.},label={lst:language-rules}, escapeinside={(*@}{@*)}, literate={\$}{{\$}}1 {@}{{@}}1, columns=flexible, keepspaces=true]
%type <node> model_stmt invoke_stmt
%type <ident> TOK_MODEL_INPUT TOK_MODEL_OUTPUT

global : use
       | function
       /* ... demais casos ... */
       | model_stmt ';'

stmt :
     /* ... demais casos ... */
     | model_stmt ';'
     | invoke_stmt ';'

factor :
       /* ... demais casos ... */
       | TOK_MODEL_OUTPUT[id] '[' expr ']' { 
         // Extract model name from "model.output"
         std::string fullName($id);
         std::string modelName = fullName.substr(0, fullName.find('.'));
         $$ = new ModelNode(modelName.c_str(), "output", $expr, nullptr, @id);
         $$->setLocation(@id); 
       }

model_stmt : TOK_MODEL TOK_IDENTIFIER '(' paramscall ')' (*@ \{ @*)
    $$ = new ModelNode($2, $4, @2);
(*@ \} @*)

invoke_stmt : TOK_INVOKE TOK_IDENTIFIER (*@ \{ @*)
    $$ = new ModelNode($2, "invoke", @2);
(*@ \} @*)

complexvar_set : TOK_MODEL_INPUT[id] '[' expr ']' '=' logicexpr { 
	// Extract model name from "model.input"
	std::string fullName($id);
	std::string modelName = fullName.substr(0, fullName.find('.'));
	$$ = new ModelNode(modelName.c_str(), "input", $expr, $logicexpr, @id);
	$$->setLocation(@id); 
}
\end{lstlisting}


\subsection{Análise semântica}
A análise semântica no Robcmp é realizada por diversos arquivos C++ que são responsáveis por verificar as regras semânticas para diferentes tipos de nós na AST. Para criar as regras semânticas relacionadas aos modelos TinyML, foram criados os arquivos \texttt{ModelNode.h} e \texttt{ModelNode.cpp}, que definem a classe \texttt{ModelNode}. Essa classe herda da classe base \texttt{Node} e implementa os métodos necessários para a verificação semântica, geração de código LLVM-IR e outras funcionalidades.

O método mais importante dentro das classes/tipos de nós do Robcmp é o \texttt{generate}. Ele funciona como um visitor e é chamado pelo compilador ao percorrer a AST. No caso do \texttt{ModelNode}, dependendo das características do nó, o \texttt{generate} gera o código LLVM-IR correspondente para a declaração do modelo, invocação do modelo ou manipulação dos tensores de entrada e saída.

O comportamento da análise semântica se divide em duas operações principais: o processamento da declaração de um modelo (\texttt{model\_stmt}) e o processamento do acesso aos seus membros (\texttt{.input}, \texttt{.output} e \texttt{invoke}). O método \texttt{generate} inicial é visto no \hyperref[lst:modelnode-generate]{Código~\ref*{lst:modelnode-generate}}, onde é decidido qual operação será realizada com base na existência dos \texttt{params}, que são os parâmetros: o nome do arquivo do modelo, o \texttt{tensor\_arena} e a lista de \textit{kernels}. A presença desses parâmetros indica uma declaração de modelo, enquanto a ausência deles indica um acesso aos membros, que também inclui a invocação do modelo.

\begin{lstlisting}[style=cpp, caption={Método \texttt{generate} da classe \texttt{ModelNode}.}, escapeinside={(*@}{@*)}, label={lst:modelnode-generate}]
Value* ModelNode::generate(FunctionImpl *func, BasicBlock *block, BasicBlock *allocblock) {
    if (!getScope()) (*@ \{ @*)
        setScope(func);
    (*@ \} @*)
    
    if (params) (*@ \{ @*)
        return generateDeclaration(func, block, allocblock);
    (*@ \} @*) else if (!memberName.empty()) (*@ \{ @*)
        return generateMemberAccess(func, block, allocblock);
    (*@ \} @*)
    return nullptr;
(*@ \} @*)
\end{lstlisting}


\subsubsection{Processamento da Declaração do Modelo (\texttt{model\_stmt})}
Esse método é responsável por gerar o código LLVM-IR necessário para declarar e inicializar uma instância do modelo TinyML. Ele realiza as seguintes etapas principais:

\begin{enumerate}

  \item Validação do Arquivo de Modelo: o compilador verifica se o arquivo do modelo especificado nos parâmetros existe e é acessível. Caso contrário, um erro semântico é gerado.
  \item Validação dos outros Parâmetros: o compilador verifica se os parâmetros \texttt{arena\_size} e \texttt{kernels} existem e são válidos. Caso contrário, um erro semântico é gerado.
  \item Geração de código LLVM-IR: cria várias variáveis globais que armazenam os dados do modelo, tamanho dos dados do modelo, \texttt{tensor\_arena}, instância do interpretador e o array de \textit{kernels}.
  \item Chamada da função do \textit{wrapper} C \texttt{InitializeInterpreter}: o compilador gera uma chamada para a função \texttt{InitializeInterpreter} do \textit{wrapper} C, passando os parâmetros necessários, e armazena o \textit{handle} retornado.
  \item Retorna o \textit{handle} do modelo: o método retorna o \textit{handle} do modelo, que será utilizado em chamadas subsequentes para manipular o modelo.

\end{enumerate}

\subsubsection{Processamento de Acesso a Membros (\texttt{.input}, \texttt{.output}, \texttt{.invoke})}
Esse método é responsável por rotear o acesso aos membros depenendo do nome do membro acessado. Inicialmente o método verifica se o modelo foi declarado anteriormente, caso contrário, um erro semântico é gerado. Em seguida, dependendo do nome do membro acessado, o método chama uma das três funções específicas: \texttt{generateInputAccess}, \texttt{generateOutputAccess} ou \texttt{generateInvoke}. Caso o nome do membro não seja reconhecido, um erro semântico é gerado, conforme mostrado no \hyperref[lst:generate-member-access]{Código~\ref*{lst:generate-member-access}}.


\begin{lstlisting}[style=cpp, caption={Roteamento do método \texttt{generateMemberAccess}.}, escapeinside={(*@}{@*)}, label={lst:generate-member-access}]
    if (memberName == "input") (*@ \{ @*)
        return generateInputAccess(func, block, allocblock, modelInstance);
    (*@ \} @*) else if (memberName == "output") (*@ \{ @*)
        return generateOutputAccess(func, block, allocblock, modelInstance);
    (*@ \} @*) else if (memberName == "invoke") (*@ \{ @*)
        return generateInvoke(func, block, allocblock, modelInstance);
    (*@ \} @*) else (*@ \{ @*)
        yyerrorcpp("Membro '" + memberName + "' nao suportado no modelo '" + modelName + "'. Use 'input', 'output' ou 'invoke'.", this);
        setSemanticError();
        return nullptr;
    (*@ \} @*)
\end{lstlisting}

\subsubsection{\texttt{generateInputAccess} (\texttt{.input})}
Este método é responsável por realizar verificações quanto ao índice fornecido e à natureza do valor atribuído ao tensor de entrada. De acordo com as verificações, decide qual método de cópia de dados deve ser chamado. O método realiza as seguintes etapas principais:

\begin{enumerate}
    \item Verifica se existe um valor a direita da atribuição: caso não exista o \textit{assignedValue}, um erro semântico é gerado. Afinal, apenas atribuições são permitidas para o tensor de entrada.
    \item Prepara o índice do tensor: caso o índice não seja fornecido, o valor padrão é 0.
    \item Chama a função \texttt{GetInputTensor} e \texttt{GetTensorSize} do \textit{wrapper} C: o compilador gera chamadas para essas funções, passando o \textit{handle} do modelo e o índice do tensor, para obter o ponteiro para o tensor de entrada e seu tamanho.
    \item Decide entre copiar um array ou um valor único: se o \textit{assignedValue} for um array, é gerada uma chamada para o método \texttt{generateVariableArrayToTensorCopy}. Caso contrário, gera uma chamada para o método \texttt{generateScalarToTensorCopy}. Ambos métodos são responsáveis por gerar o código LLVM-IR para copiar os dados do valor atribuído para o tensor de entrada.
    \item Tratamento de erros: em várias etapas o método detecta falhas de geração do código (índice, dados, etc), registra o erro semântico e retorna \texttt{nullptr}.
\end{enumerate}

\subsubsection{\texttt{generateOutputAccess} (\texttt{.output})}
Este método é responsável por gerar o código LLVM-IR responsável por resgatar valores do tensor de saída do modelo. Ele se assemelha bastante ao método \texttt{generateInputAccess}, com algumas diferenças importantes. O método realiza as seguintes etapas principais:

\textbf{COMENTÁRIO LUIZ: Ainda está errada essa subsection, aguardando o problema no output ser resolvido}

\begin{enumerate}
    \item Prepara o índice do tensor: caso o índice não seja fornecido, o valor padrão é 0.
    \item Chama a função \texttt{GetOutputTensor} do \textit{wrapper} C: o compilador gera chamadas para essa função, passando o \textit{handle} do modelo e o índice do tensor, para obter o ponteiro para o tensor de saída.
    \item Chama a função \texttt{AllocAndGetTensorArray} do \textit{wrapper} C: essa função recebe o ponteiro do tensor de saída, quantidade de elementos a serem resgatados (que pode ser nulo) e retorna um ponteiro para um array alocado dinamicamente contendo os valores em ponto flutuante.
    \item Retorna o ponteiro do array: o método retorna o ponteiro para o array alocado dinamicamente.
\end{enumerate}

\subsubsection{\texttt{generateInvoke} (\texttt{invoke})}
Este método é responsável por gerar o código LLVM-IR necessário para invocar a inferência do modelo. Ele realiza as seguintes etapas principais:

\begin{enumerate}
    \item Verifica se existe uma atribuição de valor: caso exista um \textit{assignedValue}, um erro semântico é gerado. Afinal, a invocação do modelo não deve ter um valor atribuído.
    \item Chama a função \texttt{InvokeInterpreter} do \textit{wrapper} C: o compilador gera uma chamada para essa função, passando apenas o \textit{handle} do modelo.
    \item Retorna \texttt{nullptr}: como a invocação do modelo não produz um valor diretamente, o método retorna \texttt{nullptr}.
\end{enumerate}

\section{Considerações Finais}

\subsection{Limitações}
Apesar da implementação ser funcional, algumas partes apresentam certas limitações e desvantagens que poderiam ter sido aprimoradas. No entanto, devido ao tempo disponível e ao grande escopo do projeto, algumas decisões foram tomadas visando a conclusão do trabalho.

Um aspecto que pode ser melhorado é a convenção adotada para entrada e saída de dados nos modelos. Atualmente, caso o desenvolvedor utilize variáveis ou vetores de ponto flutuante, ele deixará a responsabilidade de conversão e quantização para o \textit{wrapper}. Essa abordagem simplifica o uso da solução ao encapsular toda a a complexidade, porém, pode introduzir algumas problemáticas. Por exemplo, ao utilizar métodos de arredondamento simples durante a quantização ou conversão de tipos, pode ocorrer perda de precisão, especialmente em aplicações sensíveis a erros.

Outra limitação está relacionada aos tokens que tratam o acesso aos tensores de entrada e saída de um modelo. Eles exigem uma sintaxe específica: um identificador seguido de ``.input'' ou ``.output''. Isso restringe a linguagem RL, impedindo que campos chamados \texttt{input} e \texttt{output} sejam declarados em tipos definidos pelo usuário (\textit{types}, na terminologia da RL). O ideal seria não criar esses tokens na linguagem, mas sim tratar o acesso aos tensores como membros normais de um tipo definidos pelo usuário. Dessa forma, durante a analise semântica, o compilador verificaria se o identificador é um modelo, e então permitiria o acesso aos membros \texttt{input} e \texttt{output} normalmente.

Por fim, a classe \texttt{ModelNode} concentra uma grande quantidade de responsabilidades e utiliza um esquema de roteamento interno para decidir qual operação realizar. Isso resulta em um código menos modular e mais acoplado. O ideal seria dividir cada funcionalidade em classes separadas, como \texttt{ModelDeclarationNode}, \texttt{ModelInvokeNode}, \texttt{ModelInputNode} e \texttt{ModelOutputNode}.

Algumas dessas limitações podem ser melhoradas em trabalhos futuros. Isso inclui um refatoramento do fluxo do \textit{frontend} e a criação de novas estratégias no \textit{wrapper} para contornar a perda de precisão durante a quantização.

\subsection{Portabilidade e Suporte a Diferentes Arquiteturas}
Este trabalho tem como objetivo inicial implementar o suporte para o TFLM na arquitetura ARM Cortex-M, utilizada por diversas famílias de microcontroladores, como a STM32. No entanto, o desenvolvedor pode escolher a arquitetura desejada para gerar o \textit{firmware}, desde que o Robcmp e o TFLM ofereçam suporte à plataforma escolhida. Para isso, é necessário compilar tanto o \textit{wrapper} quanto a biblioteca do TFLM para a arquitetura específica, conforme demonstrado na \hyperref[sec:ambiente-experimental]{Seção~\ref*{sec:ambiente-experimental}}. Dessa forma, o sistema pode ser adaptado para diferentes plataformas, bastando seguir o procedimento de compilação adequado.
